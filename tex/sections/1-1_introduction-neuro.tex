%!TEX root = ../prelims_main.tex

\subsection{Neural mechs}

Until now our very simple model has been entirely theoretical, describing the general requirements of the computation of phonetic category identity, but the form of any biological computation is necessarily constrained by the substrate of its implementation (roughly, Marr's levels, for a recent discussion see \cite{rooijTheoryTestHow2020}). Though the model could be retained in its current form by recasting $\mathbf{P}$ as the neural representation of perceptual dimensions from which category $c \in \mathbf{C}$ is inferred, this would require strong assumptions about the form of the neural representation of perceptual dimensions, and in a practical modeling context assumes we have enough information to infer it. To preserve generality at the cost of complexity, we add an additional "layer" to the model, 

\begin{equation}
\mathbf{n} = \{n_0, n_i, ... n_{dn} : n \in \mathbf \mathbf{N}^{dm} \subseteq \mathbb{R}^{dn}\}
\end{equation}

where a neural state $\mathbf{n}$, a $dn$-dimensional instantaneous firing rate of neurons $n_i$ in some neural manifold $\mathbf{N}^{dm}$ of dimension $dm$ embedded within $\mathbb{R}^{dn}$. The manifold embedding $N$ reflects the intrinsic constrants network structure poses on the possible states $\mathbf{n} \in \mathbf{N} \subseteq \mathbb{R}$, but the embedding is arbitrary.

The neural layer is incorporated by modifying equation \ref{eqn:w} such that

\begin{align}
M_n &= f(\mathbf{s}, \mathbf{p}) : \mathbf{S} \rightarrow \mathbf{N}\\
M_p &= f(\mathbf{n}) : \mathbf{N} \rightarrow \mathbf{P}
\end{align}

where some sensory input $\mathbf{s}$ is mapped to some neural state $\mathbf{n}$, which supports some percept $\mathbf{p}$ from which phonetic category is computed. The dependence of $M_n$ on $\mathbf{p}$ reflects the possibility of top-down influence on the neural representation of a given stimulus. \todo{talk about the representation of time in the model}. 

\draft{note that what we're doing here is largely accounting for incomplete observation and agnosticism of the implementation of perceptual representation. For example there might be some real perceptual dimension that is not independently represented in the neural space, but is computed "downstream" by some structure that we're not observing. In the case of making a claim on the structure of neural representation (talk about alternatives briefly, that not everything necessarily is represented by the firing rate) and full observation, $\mathbf{N} = \mathbf{P}$ -- where $\mathbf{P}$ is then the perceptual space represented by the brain from which category identity is computed. So talk about when we separate vs. when we treat them as the same in following section}


Arguably a computational strategy common to all sensory systems is to exploit regularities in the statistical structure of the natural world to form an efficient sensory representation\cite{kuhlBrainMechanismsEarly2010,kingRecentAdvancesUnderstanding2018a,smithEfficientAuditoryCoding2006a,stilpRapidEfficientCoding2010,schiavoCapacitiesNeuralMechanisms2019,barlowSingleUnitsSensation1972}(\todo{cite more here bc broad claim}). Though the task of phonetic perception is a truly monstrous one (\draft{expand more here?}), work since the heyday of motor theories has demonstrated the remarkable ability of the auditory system to perform the fundamental computations of phonetic categorization has given the problem an air of tractability. And though we still are methodologically limited in our ability to study spech perception in humans at the spatiotemporal scales of its computation, work in animal models as well as recent advances in human brain electrophysiology have given some of the first glimpses. 

Several features of our model are happily known to be true of neurons in mammalian auditory cortex. 

Neurons in primary auditory cortex jointly encode multiple dimensions of sound\cite{kingRecentAdvancesUnderstanding2018a}. In ferrets presented with an array of stimuli that varied by pitch, timbre, and azimuth\cite{bizleyInterdependentEncodingPitch2009b}, more A1 neurons were observed to be sensitive to two or three dimensions (36\% and 29\%, respectively) than a single dimension (23\%). In a subset of neurons, these responses were temporally complex such that the dimensions could be partially recovered by separating sustained from onset responses\cite{walkerMultiplexedRobustRepresentations2011}. Similar results have been observed in marmosets (combined sensitivity to amplitude modulation, frequency modulation, etc. \cite{Wang2005a}) and in studies that estimated the dimensionality of receptive fields from complex stimuli like dynamic ripples in cats\cite{atencioMultidimensionalReceptiveField2017}. This is perhaps unsurprising, as cortical neurons being sensitive to multiple dimensions of a stimulus is a trivial reformulation of the well-known hierarchical processing throughout the auditory system (for a review, see \cite{sharpeeHierarchicalRepresentationsAuditory2011b}): cortical neurons representing "higher order" properties of a stimulus necessarily implies sensitivity to multiple features of the stimulus (provided a generously-enough low-level description of the stimulus feature space).

Maciello and colleagues recently argued that joint, rather than independent encoding of multiple stimulus dimensions is computationally advantageous\cite{macellaioWhySensoryNeurons2020}. Though sensitivity to multiple features makes response patterns ambiguous with respect to the value of any individual dimension, joint encoding provides more information about all represented dimensions to a downstream decoder. If it is the case that joint encoding is constitutive of auditory representations, and individual stimulus or perceptual dimensions are never (or rarely) represented independently, behavior that reflects sensitivity to family resemblance structure rather than optimal rule-based categorization is parsimonious. If all features are estimated simultaneously, influence of "nontarget" dimensions becomes unsurprising. 

Auditory cortical neurons adapt to predictable acoustic statistics in order to represent more informative stimulus dimensions at both short and long timescales. 

A rich body of research has described the many conditions that auditory representations are modulated by context (for a review, see \cite{angeloniContextualModulationSound2018}) at timescales as short as hundreds of milliseconds\cite{deanRapidNeuralAdaptation2008b,rabinowitzContrastGainControl2011c}. Processes like forward masking (cite), stimulus-specific adaptatation (SSA, cite), and suppression of background noise all reflect the general principle that auditory representations adapt to predictable acoustic statistics (\todo{cites here}) in order to form robust, invariant representations of auditory objects\cite{rabinowitzConstructingNoiseinvariantRepresentations2013} by emphasizing the maximally informative dimensions\cite{atencioMultidimensionalReceptiveField2017}. 

Adaptation to noise or stimulus statistics can be characterized as a short-term `reweighting' of features through processes like synaptic depression\cite{mesgaraniMechanismsNoiseRobust2014,davidRapidSynapticDepression2009} or microcircuit interactions\cite{natanComplementaryControlSensory2015b,natanCorticalInterneuronsDifferentially2017}. In tasks based on simple parametric sounds, representations of task-relevant stimuli are enhanced on the order of minutes\cite{fritzRapidTaskrelatedPlasticity2003a}. Animals trained on multiple tasks had neurons that adapted their receptive fields to facilitate the different task demands\cite{fritzActiveListeningTaskdependent2005b} and reward structures\cite{davidTaskRewardStructure2012}. David and Shamma (2013\cite{davidIntegrationMultipleTimescales2013}) argue that short-term integration of auditory context could also be a substrate for representing and comparing auditory features that occur through time. 

The auditory system is also plastic on longer timescales to represent the dimensions of sound that are maximally informative to the demands placed on it. Rats trained using a single set of stimuli had differential enhancement of sensitivity to frequency or intensity depending on which they were trained to attend to\cite{Polley2006}. Bieszczad and Weinberger observed that such enhancement correlated with the strength of a learned memory trace\cite{bieszczadRepresentationalGainCortical2010}. 

\textbf{speech-specific stuff}

Human and animal studies make suggestions about the nature of the representation of speech sounds in auditory cortex. 

A series of studies from Edward Chang and colleagues recording electrophysiological activity in human temporal lobe using high-density multi-electrode arrays have contributed greatly to our understanding of the encoding of speech sounds, particularly in the superior temporal gyrus (STG) (\todo{<- redundancy supreme here})\cite{yiEncodingSpeechSounds2019}. 

Recordings of high-gamma (70-150Hz) power show individual electrode sites in middle to posterior STG are selective to acoustically similar groups of phonemes (eg. obstruent vs. sonorant selectivity, plosive vs. fricative selectivity, etc.)\cite{mesgaraniPhoneticFeatureEncoding2014}. These phonetic sensitivites were reflective of sensitivity to multiple complex acoustic features that are correlated within phonetic categories. 

Responses are spatially heterogeneous\cite{mesgaraniPhoneticFeatureEncoding2014,hamiltonSpatialMapOnset2018a} --- sensitivity to specific features is not stereotyped across individuals, but there is some spatial organization. General axis of posterior STG being sensitive to onsets and anterior being sensitive to sustained component\cite{hamiltonSpatialMapOnset2018a}. Interestingly the most information seems to be present in 150ms or so after speech sound onset, which indicates that it is not instantaneous neural responses to phonemes, but the result of some temporal computation that follows a general nonspecific repsonse\cite{mesgaraniPhoneticFeatureEncoding2014}. 



features directly encoded\cite{changCategoricalSpeechRepresentation2010b,mesgaraniPhoneticFeatureEncoding2014,belinVoiceselectiveAreasHuman2000b,Pasley2012}


\draft{speech categorization is a big neurolinguistic prob\cite{yiEncodingSpeechSounds2019}}

It's all about the left anterior superior temporal gyrus\cite{yiEncodingSpeechSounds2019}. Specifically, neurons in STG encode higher-order acoustic properties that correspond to those present in categories of speech sounds (eg. frication vs. sonority, formant band combinations). Tuning isn't `clean' -- neighboring cells have dramatically different tuning, and all reflect some sort of complex spectrotemporal sensitivity (firing to specific speech sounds, but none to tones/simple sounds) (left aSTG)\cite{chanSpeechSpecificTuningNeurons2014}, and are very heterogeneous between people. combined with animal lit about developed sensitivity, it's probably the case that people learn their own basis sets for feature detection in secondary auditory cortical areas. Indeed different people have different cue weightings that are more or less adaptive\cite{clayardsDifferencesCueWeights2018}

\draft{get putative mouse "analogue" from crystal engineer's papers. emergence of invariant reps in secondary auditory cortex\cite{carruthersEmergenceInvariantRepresentation2015c}}

\draft{vocalization sensitive neurons in anterior left acx with different projection patterns from/to L6 that are experience dependent. (cfos\cite{levyCircuitAsymmetriesUnderlie2019a})}

Reciprocal connections with straitum could facilitate the plasticity in cortex b/c dopaminergic projections responsive to reward \cite{fengRoleHumanAuditory2018}

Auditory system makes efficient codes that collapse uninformative variability, and learns the statistical structure inherent in acoustic reality \cite{schiavoCapacitiesNeuralMechanisms2019} and phonetic production specifically\cite{kuhlNewViewLanguage2000} -- responses to sound become "non-isomorphic" to the acoustic features in the sound \cite{stilpEfficientCodingStatistically2012,wangNeuralCodingStrategies2007} as dimensions that are more informative than raw acoustic features are computed. *not* representing the sound precisely is more efficient than representing it directly becuase then you can take advantage of the *informative* elements of the sound rather than the ones that are spandrels of the physics of the acoustic generator.

Rats trained on speech sounds had an increased sensitivity to tones in the frequency range of the presented token\cite{engineerSpeechTrainingAlters2015a}.

\draft{when we talk representation we're not just talking about an increased firing rate, the goal of an efficient code is also to reduce firing for irrelevant shit, and decreases in activity for learned or regular sounds has been observed plenty.}
---


Lots of people already talking about this, but even criticisms sorta treat perceptual dimensions as a given, and it is the brain's fault that it doesn't represent them. \cite{goddardInterpretingDimensionsNeural2018a}

---


\begin{itemize}
\item auditory processing as domain-general and domain-specific across multiple timescales \cite{norman-haignereHierarchicalIntegrationMultiple2020}
\item abrupt transitions, at least in neural data \cite{durstewitzAbruptTransitionsPrefrontal2010}
\item other reward-learning regions like RSC \cite{millerRetrosplenialCorticalRepresentations2019}
\item multimodal representations and preserved neural manifold dynamics across inference tasks in M1 \cite{gallegoCorticalPopulationActivity2018}
\item timescales of processing expand across auditory hierarchy (and more generally have different timescales of integration and lags) \cite{norman-haignereHierarchicalIntegrationMultiple2020} and are lateralized \cite{levyCircuitAsymmetriesUnderlie2019a}
\item categorical representation of phonemes in STG, smooth gradients in F2 onset make discrete changes in linear readouts of "neural representation" \cite{changCategoricalSpeechRepresentation2010b}
\item contributions from basal ganglia in reward learning for acoustic dimentions \cite{limHowMayBasal2014}
\item this bif ol review \cite{rauscheckerMapsStreamsAuditory2009b}
\end{itemize}

probs w/ discriminatory models: how is the comparison done? eg. you could start learning features by just comparing every x thing with y thing, but then you would have to hold some representation of each in order to compare. 

\idea{start this section by introducing the necessity of having a neural implementation stage in the model, and end it by comparing to previous efforts to relate the different geometric spaces. say that assuming the featural dimensions and the neural dimensions is a central failure of geometric analysis models, like the shitty application of the second order isomorphism that is RSA, and then use that to go into the section about `so here's what i'm proposing that we do differently'}

\subsection{models}

computational. models that have attempted to explain phonetic processing??? is this its own section or what?

zoo of processing models and discussion of bayesian generativ emodels \cite{Kronrod2016a}. categorical effects are from large amount of `noise' variance, or variance on uninformative dimensions. if it's the case that there are many dimensions that have imperfect, sometimes conflicting information, then that would be reflected in categorical perception. Their discussion asks the question what effects coarticulation might have on the meaning of tau, and this is a potential one -- it could be the case that since the category structure is a family resemblance, and as such only a few of the cues are informative at a particular time, then 

\draft{relationship between generative and discriminitive models here... the means by which these features are learned is ultimately the question of implementation that grounds these orbiting ideas. How do family resemblances work? why is it possible that there are categories that operate without logical structure? why is it that we will use all the dimensions of a problem even when there is an optimal, low-dimension solution (contrast with techniques like SVM that without regularization inevitably converge on a `one true feature' that can perfectly distinguish states). what are phonemes is a question of how are they implemented. }


\subsection{scraps}

\draft{arguably the cue-theorists arrived at the wrong conclusions was because of their belief about the innateness of the auditory-perceptual mapping: it must have been genetic, so therefore language is parsimoniously some special module, etc. etc. Research based on synthesized parameters based on cues then carry that error further by not representing the full scope of the problem. like how they eventually discarded the notion of cues (definitely need more detail in that story about specific examples of how cues are conflicting in different contexts) was because they considered their interaction with other cue dimensions. If we instead take the info-theoretic perspective seriously then learning a phoneme should be the act of learning the maximally informative dimensions. since we see individual differences in cue weighting within individuals, we would also expect people's dimensions to be different... but if there is only one or a few carefully parameterized dimensions of variation present in the stimulus set, of course they'll learn those, so we need to instead use a stimulus set that preserves as much of the natural variation within category as possible and allow the animals to learn the contrastive dimensions themselves. using only two categories is of course a simplification, but it still mimics at least the nature of the learning problem in qualitative form, and also [evidence that infants learn stop consonant boundaries early and they are primary and near-universal across languages indicating that they are sorta self-stable system where the big featural distinction of being stops makes it so they are like a `submodule' within a phonetic set.]}

\begin{itemize}
	\item Short description of phonetic acoustics, why they're games
	\item General statement on importance of understanding neural implementation of a game-recognition system
	\item parameterized vs natural speech is actually reflective of a much larger positivist/naturalist philosophical divide -- they presuppose by testing a parameter of category membership, but postiive evidence is not evidence that parameter is actually constitutive of the category itself -- for example if you had two categories "games" and "cars," "weight" might be a reasonably good way to assign category membership, but it is not at all the only, or even the most salient difference between those categories. Like i feel like I'm crazy sometimes because shouldn't the fact that synthesized speech sounds \textit{sound bad} be a \textit{problem?} They might have all the theoretical justification in the world but the fact that they so badly imitate what even a plausible phoneme would sound like should be like a red flag for the generalizability of the conclusions that can be drawn from them.
	\item theoretical problems with simplified stimuli - low-dimensional and linearly-separable stimulus spaces are fundamentally different than the high complexity of naturalistic stimuli... for all we know the computations are just straight up not comparable! \cite{schuesslerInterplayRandomnessStructure2020}

\end{itemize}


levels of analysis:

phonetic perception has paradoxes at several levels of analysis that are not mutually discrete.

\textbf{ontic/algorithmic}: what \textit{are} phonemes? are they positive descriptions of combinations of features, or negative descriptions of forbidden spectrotemporal state transitions?

\textbf{implementation}: to some degree the methodological and theoretical disagreements between the feature-detection and population-computation models of phonetic perception mirror the single-cell/multicellular computation dichotomy described in the introduction of \cite{dubreuilComplementaryRolesDimensionality2020}. 

\begin{itemize}
	\item speed of processing vs. variability within category
	\item neurons that process auditory information at phonetic timescales are relatively insensitive to spectral quality \cite{norman-haignereHierarchicalIntegrationMultiple2020}
\end{itemize}


actually `warping' perceptual space relative to acoustic space is already a really common idea in phonetics lit\cite{iversonInfluencesPhoneticIdentification1996,kuhlNewViewLanguage2000} and is a sorta trivial reformulation of the idea that the auditory system is learning to represent the maximally informative dimensions of the stimulus, so a perceptual warping is just a reflection of the condensation of representation of within-category variation (ie. not being represented/generalized over/compressed/whatever you want to call it) and a maximization of representation of the between-category variation. Accounts of exemplars and stimulus geometry are complementary here: saying that perceptual space is clustered near examplars and sparser away from them is the same thing as saying they are embedded in a space whose dimensions that maximize inter-category discriminability. Put another way, instances where there is not a clear examplar to `warp' perceptual space (as in the `low-r' group in \cite{iversonInfluencesPhoneticIdentification1996}) could also correspond to the absence of a clear perceptual dimension structure within the presented stimulus space: maybe those listeners discriminability feature dimensions don't feature F3 prominently, and in instances where clear exemplars warp the perceptual space, those dimensions are emphasized by increasing the weight of existing feature dimensions, or the perceptual space is `rotated' to emphasize them. 

\end{multicols}