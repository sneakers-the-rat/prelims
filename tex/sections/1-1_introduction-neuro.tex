%!TEX root = ../prelims_main.tex

\subsection{Neural mechs}

Until now our very simple model has been entirely theoretical, describing the general requirements of the computation of phonetic category identity, but the form of any biological computation is necessarily constrained by the substrate of its implementation (roughly, Marr's levels, for a recent discussion see \cite{rooijTheoryTestHow2020}). Though the model could be retained in its current form by recasting $\mathbf{P}$ as the neural representation of perceptual dimensions from which category $c \in \mathbf{C}$ is inferred, this would require strong assumptions about the form of the neural representation of perceptual dimensions, and in a practical modeling context assumes we have enough information to infer it. To preserve generality at the cost of complexity, we add an additional "layer" to the model, 

\begin{equation}
\mathbf{n} = \{n_0, n_i, ... n_{dn} : n \in \mathbf \mathbf{N}^{dm} \subseteq \mathbb{R}^{dn}\}
\end{equation}

where a neural state $\mathbf{n}$, a $dn$-dimensional instantaneous firing rate of neurons $n_i$ in some neural manifold $\mathbf{N}^{dm}$ of dimension $dm$ embedded within $\mathbb{R}^{dn}$. The manifold embedding $N$ reflects the intrinsic constrants network structure poses on the possible states $\mathbf{n} \in \mathbf{N} \subseteq \mathbb{R}$, but the embedding is arbitrary.

The neural layer is incorporated by modifying equation \ref{eqn:w} such that

\begin{align}
M_n &= f(\mathbf{s}, \mathbf{p}) : \mathbf{S} \rightarrow \mathbf{N}\\
M_p &= f(\mathbf{n}) : \mathbf{N} \rightarrow \mathbf{P}
\end{align}

where some sensory input $\mathbf{s}$ is mapped to some neural state $\mathbf{n}$, which supports some percept $\mathbf{p}$ from which phonetic category is computed. The dependence of $M_n$ on $\mathbf{p}$ reflects the possibility of top-down influence on the neural representation of a given stimulus. \todo{talk about the representation of time in the model}. 

\draft{note that what we're doing here is largely accounting for incomplete observation and agnosticism of the implementation of perceptual representation. For example there might be some real perceptual dimension that is not independently represented in the neural space, but is computed "downstream" by some structure that we're not observing. In the case of making a claim on the structure of neural representation (talk about alternatives briefly, that not everything necessarily is represented by the firing rate) and full observation, $\mathbf{N} = \mathbf{P}$ -- where $\mathbf{P}$ is then the perceptual space represented by the brain from which category identity is computed. So talk about when we separate vs. when we treat them as the same in following section}

\draft{more on levels of analysis here? The inextricability of talking about implementation and theory is precisely reflected in the obligation of understanding the ways that the particular system results in the idiosyncracies of the observable behavior -- or the degree to which an explanation of the implementation explains and recapitulates the idiosyncracies of the observable behavior is the degree to which it is more or less "correct", in a strict modeling sense. So, precisely for the same reason that we care that our theoretical model accurately describes observable behavior, it is impossible to separate a theoretical model from its implementation -- though the temporary illusion is invaluable.}

Arguably a computational strategy common to all sensory systems is to exploit regularities in the statistical structure of the natural world to form an efficient sensory representation\cite{kuhlBrainMechanismsEarly2010,kingRecentAdvancesUnderstanding2018a,smithEfficientAuditoryCoding2006a,stilpRapidEfficientCoding2010,schiavoCapacitiesNeuralMechanisms2019,barlowSingleUnitsSensation1972}(\todo{cite more here bc broad claim}). Though the task of phonetic perception is a truly monstrous one (\draft{expand more here?}), work since the heyday of motor theories has demonstrated the remarkable ability of the auditory system to perform the fundamental computations of phonetic categorization has given the problem an air of tractability. And though we still are methodologically limited in our ability to study spech perception in humans at the spatiotemporal scales of its computation, work in animal models as well as recent advances in human brain electrophysiology have given some of the first glimpses. 

Several features of our model are happily known to be true of neurons in mammalian auditory cortex. 

Neurons in primary auditory cortex jointly encode multiple dimensions of sound\cite{kingRecentAdvancesUnderstanding2018a}. In ferrets presented with an array of stimuli that varied by pitch, timbre, and azimuth\cite{bizleyInterdependentEncodingPitch2009b}, more A1 neurons were observed to be sensitive to two or three dimensions (36\% and 29\%, respectively) than a single dimension (23\%). In a subset of neurons, these responses were temporally complex such that the dimensions could be partially recovered by separating sustained from onset responses\cite{walkerMultiplexedRobustRepresentations2011}. Similar results have been observed in marmosets (combined sensitivity to amplitude modulation, frequency modulation, etc. \cite{Wang2005a}) and in studies that estimated the dimensionality of receptive fields from complex stimuli like dynamic ripples in cats\cite{atencioMultidimensionalReceptiveField2017}. This is perhaps unsurprising, as cortical neurons being sensitive to multiple dimensions of a stimulus is a trivial reformulation of the well-known hierarchical processing throughout the auditory system (for a review, see \cite{sharpeeHierarchicalRepresentationsAuditory2011b}): cortical neurons representing "higher order" properties of a stimulus necessarily implies sensitivity to multiple features of the stimulus (provided a generously-enough low-level description of the stimulus feature space).

Maciello and colleagues recently argued that joint, rather than independent encoding of multiple stimulus dimensions is computationally advantageous\cite{macellaioWhySensoryNeurons2020}. Though sensitivity to multiple features makes response patterns ambiguous with respect to the value of any individual dimension, joint encoding provides more information about all represented dimensions to a downstream decoder. If it is the case that joint encoding is constitutive of auditory representations, and individual stimulus or perceptual dimensions are never (or rarely) represented independently, behavior that reflects sensitivity to family resemblance structure rather than optimal rule-based categorization is parsimonious. If all features are estimated simultaneously, influence of "nontarget" dimensions becomes unsurprising. 

Auditory cortical neurons adapt to predictable acoustic statistics in order to represent more informative stimulus dimensions at both short and long timescales. 

A rich body of research has described the many conditions that auditory representations are modulated by context (for a review, see \cite{angeloniContextualModulationSound2018}) at timescales as short as hundreds of milliseconds\cite{deanRapidNeuralAdaptation2008b,rabinowitzContrastGainControl2011c}. Processes like forward masking (cite), stimulus-specific adaptatation (SSA, cite), and suppression of background noise all reflect the general principle that auditory representations adapt to predictable acoustic statistics (\todo{cites here}) in order to form robust, invariant representations of auditory objects\cite{rabinowitzConstructingNoiseinvariantRepresentations2013} by emphasizing the maximally informative dimensions\cite{atencioMultidimensionalReceptiveField2017}. 

Adaptation to noise or stimulus statistics can be characterized as a short-term `reweighting' of features through processes like synaptic depression\cite{mesgaraniMechanismsNoiseRobust2014,davidRapidSynapticDepression2009} or microcircuit interactions\cite{natanComplementaryControlSensory2015b,natanCorticalInterneuronsDifferentially2017}. In tasks based on simple parametric sounds, representations of task-relevant stimuli are enhanced on the order of minutes\cite{fritzRapidTaskrelatedPlasticity2003a}. Animals trained on multiple tasks had neurons that adapted their receptive fields to facilitate the different task demands\cite{fritzActiveListeningTaskdependent2005b} and reward structures\cite{davidTaskRewardStructure2012}. David and Shamma (2013\cite{davidIntegrationMultipleTimescales2013}) argue that short-term integration of auditory context could also be a substrate for representing and comparing auditory features that occur through time. 

The auditory system is also plastic on longer timescales to represent the dimensions of sound that are maximally informative to the demands placed on it. Rats trained using a single set of stimuli had differential enhancement of sensitivity to frequency or intensity depending on which they were trained to attend to\cite{Polley2006}. Bieszczad and Weinberger observed that such enhancement correlated with the strength of a learned memory trace\cite{bieszczadRepresentationalGainCortical2010}. 

\textbf{speech-specific stuff}

The Superior Temporal Gyrus (STG) in humans, or secondary parabelt regions in some other species, of auditory cortex is the primary candidate for representation of higher-order auditory features used in speech perception. Damage to the left posterior Superior Temporal Gyrus, containing BA 22 "Wernicke's area," has long been associated with receptive aphasia, but a variety of human and animal studies have given further insight on the character of speech processing within the STG.

A series of studies from Edward Chang and colleagues recording electrophysiological activity in human temporal lobe using high-density multi-electrode arrays have contributed greatly to our understanding of the encoding of speech sounds, particularly in the superior temporal gyrus (STG) (\todo{<- redundancy supreme here})\cite{yiEncodingSpeechSounds2019}. 

Recordings of high-gamma (70-150Hz) power show individual electrode sites in middle to posterior STG are selective to acoustically similar groups of phonemes (eg. obstruent vs. sonorant selectivity, plosive vs. fricative selectivity, etc.) in humans passively listening to natural speech samples\cite{mesgaraniPhoneticFeatureEncoding2014}. These phonetic sensitivites were reflective of sensitivity to multiple complex acoustic features that are correlated within phonetic categories and that "maximiz[e] vowel discriminability in the neural domain."\cite{mesgaraniPhoneticFeatureEncoding2014}. Lower frequency (<50Hz) macrocortigraphy recordings also show that subpopulations of pSTG neurons carry information that allows discrimination of consonant-vowel token category analogously to behavioral categorization\cite{changCategoricalSpeechRepresentation2010b}. 

In the anterior STG (aSTG), individual sorted units recorded from one person demonstrated complex, speech-specific reponses when one subject was presented with a wide array of sounds\cite{chanSpeechSpecificTuningNeurons2014}. Many (66 of 141) units demonstrated selectivity to one or a few words that was invariant across speaker. Speech selectivity was only partially explained by a linear combination of acoustic features (linear spectrograms and MFCCs), and did not (over-)generalize to noise-vocoded speech, time-reversed speech. Unit responses to individual phonemes also differed by the recent phonetic past, all together suggesting that some units in aSTG are selective to the fine spectrotemporal structure of speech sounds at single-to-few phoneme timescales \cite{chanSpeechSpecificTuningNeurons2014}. 

Though acoustic response profiles are spatially heterogeneous across the STG and between individuals\cite{mesgaraniPhoneticFeatureEncoding2014,hamiltonSpatialMapOnset2018a}, there does appear to be some functional distinction between anterior and posterior STG with respect to speech sound processing. In macroelectrode recordings in humans listening to natural sentences, pSTG electrodes selectively track phrase-level onsets, while aSTG electrodes have more sustained responses through a phrase. The dissociation between onset and sustained responses was not reflective of the discontinuous vs. continuous nature of consonants and vowels, as selectivity to groups of phonemes (vowels, plosives, nasals, etc.) was mixed in both anterior and posterior STG \cite{hamiltonSpatialMapOnset2018a}. Information useful for discrimination of phonetic identity in the pSTG develops and reaches a peak 100-150ms or so after speech sound onset\cite{mesgaraniPhoneticFeatureEncoding2014,changCategoricalSpeechRepresentation2010b}, and neural state space projections onto axes representing the activity of neurons sensitive to sound onset or sustained sound show a reliable sweep between posterior and anterior STG on the order of seconds. \todo{summarize description of temporal processing distributed across multiple regions that potentially reflects different parts of the information being reflected in different... codes.}

Animal research of neural mechanisms of speech sound processing is quite sparse, and so our understanding is relatively coarse and by analogy from more general auditory research. Speech training in rats evokes a complex set of changes to acoustic response properties in several auditory cortical fields loosely analogous to secondary cortical areas in humans\cite{engineerSpeechTrainingAlters2015a}. Neurons in the anterior auditory field (AAF) and A1 were more responsive to the initial consonant in consonant-vowel (/CV/) pairs in trained vs. control rats (27\% and 57\% more spiking activity, respectively). Additionally, the proportion of neurons that were responsive to 2kHz tones (the spectral peak in the speech tokens used) increased by 65\% in AAF and 38\% in A1 after speech training compared to control rats. In contrast, in response to vowels VAF and PAF were less responsive following speech training (42\% and 30\% fewer spikes, respectively, vs. controls). In neurons that had similar frequency tuning, responses to consonants were more correlated in AAF and VAF, and responses to vowels were less correlated in AAF, A1, and VAF after speech training (vs. controls)\cite{engineerSpeechTrainingAlters2015a}. 

These results\cite{engineerSpeechTrainingAlters2015a} may not establish definitive roles for secondary auditory fields in rodent auditory cortex, but in sum do suggest that speech training induces long-lasting plasticity in auditory cortex, and suggests that processing may be distinct for different acoustic features in anterior vs. posterior fields as in humans. Mice trained to discriminate speech sounds were returned to chance following lesions of auditory cortex\cite{saundersMiceCanLearn2019}, indicating its necessity. Task-specific plasticity\cite{takahashiLearningstagedependentFieldspecificMap2011} and contribution to processing task-relevant auditory stimulus categories\cite{shiAnteriorAuditoryField2019} has been previously demonstrated in AAF, which is thought to operate as a parallel processing system, with response latencies comparable to or lower than A1 in cats\cite{carrascoNeuronalActivationTimes2011} and mice\cite{lindenSpectrotemporalStructureReceptive2003b}. PAF is a secondary auditory cortical area and thought to be downstream from both A1 and AAF\cite{pandyaSpectralTemporalProcessing2008,carrascoEvidenceHierarchicalProcessing2009}. Though their functional specialization of computational role might not be equivalent in humans, it is parsimonious to assume that primary and secondary auditory cortical areas in nonhuman mammalian auditory systems process acoustic information in such a way that supports the recognition of phonetic identity. 

\todo{Talk about the categorical decisionmaking process downstream, implications for role of nonauditory, frontal, etc. zones that actually do the integration with syntactical, semantic information. Differentiate that we're concerned about the derivation of the acoustic level, the perceptual dimensions that facilitate, but may not constitute the identity of a phoneme.}

\subsection{uh is there a name for the conclusion of an introduction because i need to make a section break to write it lmao}

In lightly constraining the constitution of $\mathbf{N}$, loosely the neural "representation" of phonetic information, the human and animal results hint at the dissociation between $\mathbf{N}$ and $\mathbf{P}$ in our model --- en passant to \draft{the statement of the research problem.} 

Suppose that one dimension $b_{vot} \in \mathbf{P}$ is the voice onset time, which dissociates voiced from unvoiced consonants (eg. /b/ vs. /p/) as the time between the onset of phonation and the occlusion of the stop. Further suppose a neural system analogous to the temporal landmark model suggested by \cite{hamiltonSpatialMapOnset2018a} where the high-energy plosive of the occlusion is "encoded" by the activity of some region analogous to the phrase-onset sensitivity of pSTG, and the sonorant, spectral quality of the voicing is encoded by another region. In this scheme, some downstream region \todo{(really need to give a name to the "readout" part of the model)} infers VOT by comparing the relative timing of gross spiking activity between these two regions. In this hopelessly na\"ive instantiation of our model, the dimension $b_{vot}$ is some real-valued (though not necessarily linear) value from negative to positive voice onset times. Such a dimension is not present in $\mathbf{N}$ as characterized by the n-dimensional space of, say, instantaneous firing rate of n neurons, requiring $M_p$, the mapping between them. 

The dissociation of the descriptions of $\mathbf{N}$ and $\mathbf{P}$ thus, in our model, defines the research problem: \textit{understanding the neural mechanisms of auditory-phonetic processing is describing the neural manifold $\mathbf{N}$ and the perceptual manifold $\mathbf{P}$ such that the difference between $\mathbf{N}$ and $\mathbf{P}$ is minimized.} It is not necessarily the case that we should expect to find neurons, or even collections of neurons, whose time-averaged firing rate is the literal measurement of the perceptual dimensions used to compute phonetic identity. Roughly, kept independent, $\mathbf{P}$ is the level of "representation" -- the basis from which the brain derives its use of phonetic information \draft{(though not necessarily unique, as the same information is represented at multiple scales, eg. like semantics lol.)}. In other words, we want to describe the neural of the perceptual dimensions that are flexibly combined to compute phonetic identity. 

This distinction may read as trivial, but it precludes a majority of the common methodological kinks of contemporary cognitive neuroscience. The implicit assumption of "decoding"-based analysis strategies is that neural representation is encoded in the language of time-averaged firing rate, and that the accuracy of some (usually uninterrogated) classification algorithm on the timeseries of firing rates (or BOLD level, or EEG bandpass amplitude, etc.) is reflective of the presence or absence of category information in the data. The same assumption is made in the case of so-called "Representational Similarity Analysis," and any number of other analytical ruts that uncritically characterize the geometry of the brain and the perceptual reality it supports as euclidean spaces with the axes of whatever recording methodology is handy for the dataset. 


\draft{End section with... This is why we should be cautious and explicit about assumptions of the nature of "representation" and why we should not assume $\mathbf{N} \equiv \mathbf{P}$. It is not necessarily the case that we should expect to find neurons, or even collections of neurons, whose time-averaged firing rate is the literal measurement of the perceptual dimensions used to compute phonetic identity. Roughly, kept independent, $\mathbf{P}$ is the level of "representation" -- the basis from which the brain derives its use of phonetic information (though not necessarily unique, as the same information is represented at multiple scales, eg. like semantics lol.). Thus, if our goal is to describe the "neural representation" of phonetic information our goal is to minimize the difference between our constructions of $\mathbf{P}$ and $\mathbf{N}$. Given our formulation of the problem as the problem of the brain being to derive some perceptual basis set $\mathbf{P}$ from which it can identify phonemes, our goal, as empirical geometers, in explaining the representation of phonemes in the brain is to find some way of [constructing?] $\mathbf{P}$ and $\mathbf{N}$ such that $\mathbf{P} \simeq \mathbf{N}$ (or, more accurately, since the brain does lots of other things that aren't hearing speech, $\mathbf{P} \subset \mathbf{N}$... and on to specific aims}

\draft{when we talk representation we're not just talking about an increased firing rate, the goal of an efficient code is also to reduce firing for irrelevant shit, and decreases in activity for learned or regular sounds has been observed plenty.}


\subsection{scraps}

 find where this go -> Indeed different people have different cue weightings that are more or less adaptive\cite{clayardsDifferencesCueWeights2018}

emergence of invariant reps in secondary auditory cortex\cite{carruthersEmergenceInvariantRepresentation2015c}

\draft{vocalization sensitive neurons in anterior left acx with different projection patterns from/to L6 that are experience dependent. (cfos\cite{levyCircuitAsymmetriesUnderlie2019a})}

Auditory system makes efficient codes that collapse uninformative variability, and learns the statistical structure inherent in acoustic reality \cite{schiavoCapacitiesNeuralMechanisms2019} and phonetic production specifically\cite{kuhlNewViewLanguage2000} -- responses to sound become "non-isomorphic" to the acoustic features in the sound \cite{stilpEfficientCodingStatistically2012,wangNeuralCodingStrategies2007} as dimensions that are more informative than raw acoustic features are computed. *not* representing the sound precisely is more efficient than representing it directly becuase then you can take advantage of the *informative* elements of the sound rather than the ones that are spandrels of the physics of the acoustic generator.


\begin{itemize}
\item auditory processing as domain-general and domain-specific across multiple timescales \cite{norman-haignereHierarchicalIntegrationMultiple2020}
\item abrupt transitions, at least in neural data \cite{durstewitzAbruptTransitionsPrefrontal2010}
\item multimodal representations and preserved neural manifold dynamics across inference tasks in M1 \cite{gallegoCorticalPopulationActivity2018}
\item timescales of processing expand across auditory hierarchy (and more generally have different timescales of integration and lags) \cite{norman-haignereHierarchicalIntegrationMultiple2020} and are lateralized \cite{levyCircuitAsymmetriesUnderlie2019a}
\item contributions from basal ganglia in reward learning for acoustic dimentions \cite{limHowMayBasal2014}
\item this bif ol review \cite{rauscheckerMapsStreamsAuditory2009b}
\end{itemize}


\draft{arguably the cue-theorists arrived at the wrong conclusions was because of their belief about the innateness of the auditory-perceptual mapping: it must have been genetic, so therefore language is parsimoniously some special module, etc. etc. Research based on synthesized parameters based on cues then carry that error further by not representing the full scope of the problem. like how they eventually discarded the notion of cues (definitely need more detail in that story about specific examples of how cues are conflicting in different contexts) was because they considered their interaction with other cue dimensions. If we instead take the info-theoretic perspective seriously then learning a phoneme should be the act of learning the maximally informative dimensions. since we see individual differences in cue weighting within individuals, we would also expect people's dimensions to be different... but if there is only one or a few carefully parameterized dimensions of variation present in the stimulus set, of course they'll learn those, so we need to instead use a stimulus set that preserves as much of the natural variation within category as possible and allow the animals to learn the contrastive dimensions themselves. using only two categories is of course a simplification, but it still mimics at least the nature of the learning problem in qualitative form, and also [evidence that infants learn stop consonant boundaries early and they are primary and near-universal across languages indicating that they are sorta self-stable system where the big featural distinction of being stops makes it so they are like a `submodule' within a phonetic set.]}

\draft{parameterized vs natural speech is actually reflective of a much larger positivist/naturalist philosophical divide -- they presuppose by testing a parameter of category membership, but postiive evidence is not evidence that parameter is actually constitutive of the category itself -- for example if you had two categories "games" and "cars," "weight" might be a reasonably good way to assign category membership, but it is not at all the only, or even the most salient difference between those categories. Like i feel like I'm crazy sometimes because shouldn't the fact that synthesized speech sounds \textit{sound bad} be a \textit{problem?} They might have all the theoretical justification in the world but the fact that they so badly imitate what even a plausible phoneme would sound like should be like a red flag for the generalizability of the conclusions that can be drawn from them.}

\draft{theoretical problems with simplified stimuli - low-dimensional and linearly-separable stimulus spaces are fundamentally different than the high complexity of naturalistic stimuli... for all we know the computations are just straight up not comparable! \cite{schuesslerInterplayRandomnessStructure2020}}


\begin{itemize}
	\item neurons that process auditory information at phonetic timescales are relatively insensitive to spectral quality \cite{norman-haignereHierarchicalIntegrationMultiple2020}
\end{itemize}


\end{multicols}